---
title: "Predict behaviour"
author: "Arman Iskaliyev"
date: '24 марта 2018 г '
output: html_document
---

```{r setup, warning=FALSE, message=FALSE}
library(readr)
library(dplyr)
library(caret)
library(ggplot2)
library(purrr)
library(rpart.plot)
set.seed(42)
```

###Datasets:

* [Training data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

* [Test data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

`classe` - our outcome

###Data processing

#####Load datasets
```{r load_ds, results='hide'}
rd.file <- "./Datasets.RData"
if (file.exists(rd.file)) {
     load(file = rd.file, verbose = TRUE)
} else {
     if (!file.exists("./data")) {
          dir.create("./data")
     }
     
     if (!file.exists("./data/pml-training.csv")) {
          download.file(
               "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
               "./data/pml-training.csv"
          )
     }
     
     if (!file.exists("./data/pml-testing.csv")) {
          download.file(
               "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
               "./data/pml-testing.csv"
          )
     }
     
     col.types <- paste0(gsub("\\d+", "", paste0("n", 1:160)))
     col.types[1] <- "-"
     col.types[c(2:7, 160)] <- "c"
     col.types <- paste(col.types, collapse = "")
     
     train.df <-
          read_csv("./data/pml-training.csv", col_types = col.types)
     test.df <- read_csv("./data/pml-testing.csv", col_types = col.types)
     rm(col.types)
     
     fctr.vec <- c("classe", "user_name", "new_window")
     train.df <- modify_at(train.df, fctr.vec, as.factor)
     test.df <- modify_at(test.df, fctr.vec, as.factor)
     rm(fctr.vec)
     
     save(train.df, test.df, file = rd.file)
}
rm(rd.file)
```

#####Data overview
```{r ds_overvw}
#variables class
map_chr(test.df, class) %>% table()

dim(train.df)
dim(test.df)

levels(train.df$classe)

#map_int(train.df, function(x) sum(is.na(x))) %>% keep(~.>0)
#map_int(test.df, function(x) sum(is.na(x))) %>% keep(~.>0)
```

#####Create custom datasets
Create xy datasets and drop all non-numeric variables.
```{r subset_1}
train.x <- select_if(train.df, is.numeric)
train.y <- train.df$classe

validation.x <- select_if(train.df, is.numeric)
validation.y <- train.df$classe

test.x <- select_if(test.df, is.numeric)

#map_chr(train.df, class) %>% keep(~.=="character")
```

#####Handling missing data
100 variables in our training and test datasets contains missing values. Most of machine learning algorithms require a non missing values and we need to use some methods to handle NA values in our datasets. So widely used solution is this:

1. look at distributions of NA and use deletion strategy to drop vaiables if the data is missing for more than 60% observations
2. apply median or kNN imputation method for remaining variables
3. identify near zero-variance variables and drop them too

```{r na_handle}
#proportions of missing values in each variable
train.na.prop <- train.x %>% 
     map_dbl(~mean(is.na(.))) %>% 
     keep(~.>0) %>% 
     as_tibble() %>% 
     tibble::rownames_to_column()
nrow(train.na.prop)

test.na.prop <- test.x %>% 
     map_dbl(~mean(is.na(.))) %>% 
     keep(~.>0) %>% 
     as_tibble() %>% 
     tibble::rownames_to_column()
nrow(test.na.prop)

train.x <- select(train.x, -one_of(filter(train.na.prop, value>0.6)$rowname))
test.x <- select(test.x, -one_of(filter(test.na.prop, value>0.6)$rowname))

train.x %>% map_dbl(~mean(is.na(.))) %>% keep(~.>0)
test.x %>% map_dbl(~mean(is.na(.))) %>% keep(~.>0)

#no NA after 1st step, skip step 2

#show nzv columns
nearZeroVar(train.x, saveMetrics = TRUE) %>% tibble::rownames_to_column() %>% filter(zeroVar | nzv)
nearZeroVar(test.x, saveMetrics = TRUE) %>% tibble::rownames_to_column() %>% filter(zeroVar | nzv)

#nothing to drop 
rm(train.na.prop)
rm(test.na.prop)
```

#####Separate training dataset into 80/20 training and validation datasets
```{r valid_ds}
index.train <- createDataPartition(train.df$classe, p=0.8, list=FALSE)
train.x <- train.x[index.train,]
train.y <- train.y[index.train]

validation.x <- train.x[-index.train,]
validation.y <- train.y[-index.train]
```

#####Methods for modelling
Cause our variable of interest is categorical variable with five classes our task is called a Multiclass Classification. And commonly used methods in this case are:

* Decision trees
* Random forests or random decision forests
* Support vector machines

In my project I used only `Decision trees` and `Random Forests`.

Fit decision tree with 10-fold cross-validation :
```{r dectree_fit, warning=FALSE, message=FALSE}
model.dtree <- train(x = train.x, 
                     y = train.y, 
                     method = "rpart", 
                     parms = list(split = "gini"),
                     trControl = trainControl(method = "cv", 
                                              number = 10), 
                     tuneLength = 10)

model.dtree
```

Plot model:
```{r dectree_plot, warning=FALSE, message=FALSE}
prp(model.dtree$finalModel, box.palette = "Reds", branch = 1)
```

Prediction:
```{r dectree_pred, warning=FALSE, message=FALSE}
predicted.model.dtree <- predict(model.dtree, newdata = validation.x)
confusionMatrix(predicted.model.dtree, validation.y)
```


Random forests
```{r rf_fit, warning=FALSE, message=FALSE}
# model.rf <- train(x = train.x, 
#                   y = train.y, 
#                   method = "ranger", 
#                   trControl = trainControl(method = "cv", 
#                                            number = 5))
# 
# model.rf
```